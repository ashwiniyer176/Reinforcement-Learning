{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole Agent with OpenAI Gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:16.0\n",
      "Episode:2 Score:19.0\n",
      "Episode:3 Score:36.0\n",
      "Episode:4 Score:25.0\n",
      "Episode:5 Score:23.0\n",
      "Episode:6 Score:24.0\n",
      "Episode:7 Score:20.0\n",
      "Episode:8 Score:17.0\n",
      "Episode:9 Score:12.0\n",
      "Episode:10 Score:32.0\n",
      "Episode:11 Score:21.0\n",
      "Episode:12 Score:38.0\n",
      "Episode:13 Score:20.0\n",
      "Episode:14 Score:19.0\n",
      "Episode:15 Score:30.0\n",
      "Episode:16 Score:30.0\n",
      "Episode:17 Score:25.0\n",
      "Episode:18 Score:16.0\n",
      "Episode:19 Score:33.0\n",
      "Episode:20 Score:17.0\n"
     ]
    }
   ],
   "source": [
    "# Testing with random moves\n",
    "\n",
    "episodes = 20\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0, 1])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(f\"Episode:{episode} Score:{score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(states,actions):\n",
    "    model=tf.models.Sequential()\n",
    "    model.add(tf.layers.Flatten(input_shape=(1,states)))\n",
    "    model.add(tf.layers.Dense(24,activation=\"relu\"))\n",
    "    model.add(tf.layers.Dense(24,activation=\"relu\"))\n",
    "    model.add(tf.layers.Dense(actions,activation=\"linear\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 24)                120       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 50        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=make_model(states,actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train agent with Keras RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model,\n",
    "                   memory=memory,\n",
    "                   policy=policy,\n",
    "                   nb_actions=actions,\n",
    "                   nb_steps_warmup=10,\n",
    "                   target_model_update=1e-2)\n",
    "    return dqn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute '_compile_time_distribution_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ashwin\\Machine Learning\\Reinforcement-Learning\\CartPole\\CartPole.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ashwin/Machine%20Learning/Reinforcement-Learning/CartPole/CartPole.ipynb#ch0000012?line=0'>1</a>\u001b[0m agent\u001b[39m=\u001b[39mbuild_agent(model,actions)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ashwin/Machine%20Learning/Reinforcement-Learning/CartPole/CartPole.ipynb#ch0000012?line=1'>2</a>\u001b[0m agent\u001b[39m.\u001b[39;49mcompile(tf\u001b[39m.\u001b[39;49moptimizers\u001b[39m.\u001b[39;49mAdam(learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m),metrics\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mmae\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ashwin/Machine%20Learning/Reinforcement-Learning/CartPole/CartPole.ipynb#ch0000012?line=2'>3</a>\u001b[0m agent\u001b[39m.\u001b[39mfit(env,nb_steps\u001b[39m=\u001b[39m\u001b[39m50000\u001b[39m,visualize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\RL\\lib\\site-packages\\rl\\agents\\dqn.py:167\u001b[0m, in \u001b[0;36mDQNAgent.compile\u001b[1;34m(self, optimizer, metrics)\u001b[0m\n\u001b[0;32m    <a href='file:///~/miniconda3/envs/RL/lib/site-packages/rl/agents/dqn.py?line=163'>164</a>\u001b[0m metrics \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [mean_q]  \u001b[39m# register default metrics\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/miniconda3/envs/RL/lib/site-packages/rl/agents/dqn.py?line=165'>166</a>\u001b[0m \u001b[39m# We never train the target model, hence we can set the optimizer and loss arbitrarily.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/miniconda3/envs/RL/lib/site-packages/rl/agents/dqn.py?line=166'>167</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_model \u001b[39m=\u001b[39m clone_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_model_objects)\n\u001b[0;32m    <a href='file:///~/miniconda3/envs/RL/lib/site-packages/rl/agents/dqn.py?line=167'>168</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msgd\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='file:///~/miniconda3/envs/RL/lib/site-packages/rl/agents/dqn.py?line=168'>169</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msgd\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\RL\\lib\\site-packages\\rl\\util.py:16\u001b[0m, in \u001b[0;36mclone_model\u001b[1;34m(model, custom_objects)\u001b[0m\n\u001b[0;32m     <a href='file:///~/miniconda3/envs/RL/lib/site-packages/rl/util.py?line=10'>11</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='file:///~/miniconda3/envs/RL/lib/site-packages/rl/util.py?line=11'>12</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mclass_name\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[0;32m     <a href='file:///~/miniconda3/envs/RL/lib/site-packages/rl/util.py?line=12'>13</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39mget_config(),\n\u001b[0;32m     <a href='file:///~/miniconda3/envs/RL/lib/site-packages/rl/util.py?line=13'>14</a>\u001b[0m }\n\u001b[0;32m     <a href='file:///~/miniconda3/envs/RL/lib/site-packages/rl/util.py?line=14'>15</a>\u001b[0m clone \u001b[39m=\u001b[39m model_from_config(config, custom_objects\u001b[39m=\u001b[39mcustom_objects)\n\u001b[1;32m---> <a href='file:///~/miniconda3/envs/RL/lib/site-packages/rl/util.py?line=15'>16</a>\u001b[0m clone\u001b[39m.\u001b[39mset_weights(model\u001b[39m.\u001b[39;49mget_weights())\n\u001b[0;32m     <a href='file:///~/miniconda3/envs/RL/lib/site-packages/rl/util.py?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m clone\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\RL\\lib\\site-packages\\keras\\engine\\training_v1.py:157\u001b[0m, in \u001b[0;36mModel.get_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///~/miniconda3/envs/RL/lib/site-packages/keras/engine/training_v1.py?line=149'>150</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_weights\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///~/miniconda3/envs/RL/lib/site-packages/keras/engine/training_v1.py?line=150'>151</a>\u001b[0m   \u001b[39m\"\"\"Retrieves the weights of the model.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/miniconda3/envs/RL/lib/site-packages/keras/engine/training_v1.py?line=151'>152</a>\u001b[0m \n\u001b[0;32m    <a href='file:///~/miniconda3/envs/RL/lib/site-packages/keras/engine/training_v1.py?line=152'>153</a>\u001b[0m \u001b[39m  Returns:\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/miniconda3/envs/RL/lib/site-packages/keras/engine/training_v1.py?line=153'>154</a>\u001b[0m \u001b[39m      A flat list of Numpy arrays.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/miniconda3/envs/RL/lib/site-packages/keras/engine/training_v1.py?line=154'>155</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/miniconda3/envs/RL/lib/site-packages/keras/engine/training_v1.py?line=155'>156</a>\u001b[0m   strategy \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distribution_strategy \u001b[39mor\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/miniconda3/envs/RL/lib/site-packages/keras/engine/training_v1.py?line=156'>157</a>\u001b[0m               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compile_time_distribution_strategy)\n\u001b[0;32m    <a href='file:///~/miniconda3/envs/RL/lib/site-packages/keras/engine/training_v1.py?line=157'>158</a>\u001b[0m   \u001b[39mif\u001b[39;00m strategy:\n\u001b[0;32m    <a href='file:///~/miniconda3/envs/RL/lib/site-packages/keras/engine/training_v1.py?line=158'>159</a>\u001b[0m     \u001b[39mwith\u001b[39;00m strategy\u001b[39m.\u001b[39mscope():\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute '_compile_time_distribution_strategy'"
     ]
    }
   ],
   "source": [
    "agent=build_agent(model,actions)\n",
    "agent.compile(tf.optimizers.Adam(learning_rate=1e-3),metrics=['mae'])\n",
    "agent.fit(env,nb_steps=50000,visualize=False,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "127652920e2d0e803d7a12d8d4181a31b557de6cc79b94bfae4438defd89d1e5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
